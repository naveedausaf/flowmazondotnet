name: CI
# This concurrency block definition ensures that
# only a single instance of this workflow can be
# be running at any given time for a given
# source (i.e. head) branch
concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true
permissions:
  checks: write
  pull-requests: write
on:
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened, closed]
jobs:
  compute-non-secret-vars:
    name: Compute non-secret variables for later jobs
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    outputs:
      pr_label: ${{ steps.compute-non-secret-vars.outputs.pr_label }}
      new_version: ${{ steps.compute-non-secret-vars.outputs.new_version }}
      tf_workspace_name: ${{ steps.compute-non-secret-vars.outputs.tf_workspace_name }}
      tf_api_workspace_name: ${{ steps.compute-non-secret-vars.outputs.tf_api_workspace_name }}
      vault_secret_conn_name: ${{ steps.compute-non-secret-vars.outputs.vault_secret_conn_name }}
      psql_owner_conn_name: ${{ steps.compute-non-secret-vars.outputs.psql_owner_conn_name }}
      neon_branch_name: ${{ steps.compute-non-secret-vars.outputs.neon_branch_name }}
      vercel_project_name: ${{ steps.compute-non-secret-vars.outputs.vercel_project_name }}
      vercel_app_domain: ${{ steps.compute-non-secret-vars.outputs.vercel_app_domain }}
      grafana_instance_name: ${{ steps.compute-non-secret-vars.outputs.grafana_instance_name }}
      grafana_instance_url_secret: ${{ steps.compute-non-secret-vars.outputs.grafana_instance_url_secret }}
      otel_env: ${{ steps.compute-non-secret-vars.outputs.otel_env }}
      vercel_project_id_secret: ${{ steps.compute-non-secret-vars.outputs.vercel_project_id_secret }}
      api_resource_group_name: ${{ steps.compute-non-secret-vars.outputs.api_resource_group_name }}
      api_app_name: ${{ steps.compute-non-secret-vars.outputs.api_app_name }}
      api_app_environment_name: ${{ steps.compute-non-secret-vars.outputs.api_app_environment_name }}
      api_app_domain_name: ${{ steps.compute-non-secret-vars.outputs.api_app_domain_name }}
      vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS: ${{ steps.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}
      vault_secretname_registry_password_or_token: ${{ steps.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}
    steps:
      - name: Compute non-secret variables for this and later jobs
        id: compute-non-secret-vars
        # This step writes out those vars that need to be available
        # later in this job or in other jobs in the workflow.
        #
        # Many of these would be put in env block
        # at the top (outside of all jobs) but since some
        # of them require github context to compute the value,
        # I needed to put them within a job (in this job, as it's
        # the first one on the chaon of build and deploy jobs)
        # and output to $GITHUB_OUTPUT, then further output those
        # from outputs block from the job. This ensures the
        # computed values are available to other jobs.
        #
        # Since I am collecting such vars/computed values
        # here in this step, even the ones that don't need to
        # be output for other jobs and only need to be used
        # by other steps of this very job, and which could
        # therefore be put in `env` block of this job,
        # have been collected here. So these would now
        # need to be referenced as
        # ${{ steps.compute-non-secret-vars.outputs.<var_name> }} from
        # other steps of this job rather than as
        # ${{ env.<var_name> }} in those steps.
        # CRUCIAL: All computed values should be non-secret
        # and based on secrets GitHub context. This makes them safe to (relatively) freely
        # share with any job in the workflow.
        env:
          # putting this in env block so I can access it in step
          # using ${{ env.PR_LABEL }} rather than ${ PR_LABEL}.
          # Using env context enabled me to check if env var
          # values are as they should be if I turn on
          # debug logging (which would show a missing
          # env var as being `null`).
          PR_LABEL: pr${{ github.event.number }}
        run: |
          echo "pr_label=${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          # we don't know what the version number would be when,
          # and if, this pull request is merged to main.
          # so best to use the place holder 0.0.0 that we also
          # use in package.json in NPM packages (which says
          # 0.0.0-semanticrelease in its "version" field;
          # I do the same in .NET csproj file's <version>
          # element also).
          # Of course we want to suffix 0.0.0 with the PR
          # label for the artifacts produced by this workflow
          # which runs on the source branch of a pull request.
          # NEW_VERSION: 
          echo "new_version=0.0.0-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "tf_workspace_name=preview-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "tf_api_workspace_name=preview-api-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vault_secret_conn_name=${{ vars.VAULT_SECRETNAME_FOR_CONNECTIONSTRING_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "psql_owner_conn_name=${{ vars.SECRETNAME_FOR_PSQL_OWNER_CONNECTIONSTRING_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "neon_branch_name=${{ vars.NEON_NEW_BRANCH_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vercel_project_name=${{ env.PR_LABEL }}${{ vars.VERCEL_PROJECT_NAME_SUFFIX }}" >> $GITHUB_OUTPUT

          echo "vercel_app_domain=${{ env.PR_LABEL }}.${{ vars.VERCEL_APP_APEX_DOMAIN_NAME }}" >> $GITHUB_OUTPUT

          echo "grafana_instance_name=${{ vars.NEXT_PUBLIC_FARO_SERVICE_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "grafana_instance_url_secret=${{ vars.SECRETNAME_FOR_GRAFANACLOUD_FRONTEND_O11Y_INSTANCE_URL_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "otel_env=${{ vars.OTEL_ENVIRONMENT_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vercel_project_id_secret=${{ vars.SECRETNAME_FOR_VERCEL_PROJECT_ID_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_resource_group_name=${{ vars.API_RESOURCE_GROUP_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_name=${{ vars.API_APP_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_environment_name=${{ vars.API_APP_ENVIRONMENT_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_domain_name=api-${{ env.PR_LABEL }}.${{ vars.VERCEL_APP_APEX_DOMAIN_NAME }}" >> $GITHUB_OUTPUT

          echo "vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ vars.VAULT_SECRETNAME_ENV_OTEL_EXPORTER_OTLP_HEADERS_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          # The name of the secret in key vault in which GitHub PAT
          # or other token/secret for accessing the Container registry
          #would be sore
          echo "vault_secretname_registry_password_or_token=${{ vars.VAULT_SECRETNAME_REGISTRY_PASSWORD_OR_TOKEN_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

  audit-signatures:
    name: Audit signatures of NPM dependencies
    runs-on: ubuntu-24.04
    if: github.event.action != 'closed'
    steps:
      - uses: actions/checkout@v4
      - name: Audit Provenance Attestations and Signatures
        run: |
          npm ci
          npm audit signatures

  create-update-preview-ephemeral:
    name: Create or Update Infrastructure for PR's Preview
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: compute-non-secret-vars
    if: github.event.action != 'closed'
    steps:
      - uses: actions/checkout@v4

      - name: Create or update preview-ephemeral HCP workspace
        # We don't care if the worksapce already exists as
        # in that case this HCP API Would return an error
        # that we ignore so the step would do nothing.
        # This behaviour is useful if for whatever reason
        # the first time this workflow ran for a PR, the
        # workspace failed to be created. In that we can
        # re-run the workflow and it will try to create
        # the workspace again.
        # HENCE commenting out the line below:
        # if: github.event.action == 'opened'
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_PROJECT: ${{ secrets.TF_PROJECT }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          TF_WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        # It is CRUCIAL to set "execution-mode": "local"
        # in the "data" object in the JSON request body
        # in the cURL to create the TF workspace below.
        #
        # This ensures that terraform apply executes
        # locally even though the state file is maintained
        # in HCP Terraform.
        #
        # Without this, it would run on HCP's runner.
        # There it would fail on terraform apply (but
        # not on terraform init) with the message that modules
        # referenced using `../../modules/<module folder>`
        # could not be read:
        #
        # │ Error: Unreadable module directory
        # │
        # │ Unable to evaluate directory symlink: lstat ../../modules: no such file or
        # │ directory
        # ╵
        # ╷
        # │ Error: Unreadable module directory
        # │
        # │ The directory  could not be read for module "flowmazonfrontend" at
        # │ main.tf:1.
        # ╵
        # Operation failed: failed running terraform init (exit 1)
        # Error: Terraform exited with code 1.
        #
        #
        # You can try to fix that by using git references
        # for the two modules used by the preview-ephemeral
        # root module that corresponds to the workspace:
        #
        # module "flowmazonfrontend" {
        #   source = "git::https://github.com/EnableHub/flowmazondotnet.git//.iac/modules/flowmazonfrontend?ref=main"
        #
        # and
        #
        # module "db_branch" {
        # source = "git::https://github.com/EnableHub/flowmazondotnet.git//.iac/modules/db_branch?ref=main"
        #
        # Then you would get an error if there are more than
        # one modules being referenced via github references
        # in the root module (a single module referenced like
        # this is fine though, which is a bit baffling):
        #
        # ╷
        # │ Error: Failed to expand subdir globs
        # │
        # │ subdir ".iac/modules/flowmazonfrontend" not found
        #
        # Basically one of the modules cannot be deferenced.
        #
        # I did have the option of copying across the referenced
        # modules into the dir of the root module, then
        # modify the root module to change the references
        # dynamically in GitHub Actions workflow. This
        # would likely have worked and would have required
        # only local relative path references because the
        # module folders would be within the root module's
        # folder.
        #
        # However, I have instead chosen to execute the TF
        # config locally while keeping the state file on
        # HCP Terraform. This is acceptable as the
        # GitHub Actions runner on which terraform apply
        # runs is also a clean machine and the state is
        # still persisted in and managed by HCP Terraform.

        run: |
          curl -sS \
            -H "Authorization: Bearer ${{ env.HCP_TF_TOKEN }}" \
            -H "Content-Type: application/vnd.api+json" \
            -X POST \
            -d '{
              "data": {
          "attributes": {
            "name": "'"${{ env.TF_WORKSPACE_NAME }}"'",
            "auto-apply": false,
            "execution-mode": "local"
            
          },
          "type": "workspaces",
          "relationships": {
            "tag-bindings": {
              "data": [
                {
            "type": "tag-bindings",
            "attributes": {
              "key": "preview-ephemeral",
              "value": ""
            }
                }
              ]
            },
            "project": {
              "data": {
                "id": "'"${{ env.HCP_TF_PROJECT }}"'",
                "type": "projects"
              }
            }
          }
              }
            }' \
            "https://app.terraform.io/api/v2/organizations/${{ env.HCP_TF_ORG }}/workspaces" \
            | tee response.json

          # If workspace already exists, the API will return an error. Ignore if already exists.
          if grep -q '"errors"' response.json; then
            if grep -q 'has already been taken' response.json; then
              echo "Workspace ${{ env.TF_WORKSPACE_NAME }} already exists (that's great!). Continuing to next step..."
            else
              cat response.json
              exit 1
            fi
          fi
      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraform block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral

      # TF HCP workspaces that are executed locally, like the ephemeral
      # workspace in this can sometimes stay locked if the workflow
      # was canceleld (e.g. a new puch was made to the source branch
      # of pull request or the pull request was merged)
      # Since ephemeral workspace created/updated/deleted in this
      # workflow are only creaetd/updated/delete by this workflow, within
      # the concurrenty group of the source branch, the source branch
      # of the pull request effectively owns these worksapces.
      # Moreover, due to concurrenty group of this workflow,
      # there can only one one operation taking place on these
      # workspaces at any give time.
      # Therefor, it is safe to use the -nolock option
      # when calling terraform apply and terraform destroy
      # on any of the ephemeral worksapces in this workflow in order
      # to get around the issues that such a workspace might sometimes
      # fail to get unlocked in HCP.
      #
      # I think it is slightly safer to explicitly unlock it first
      # before use, which is what I am doing below.
      # I do the same with apply and destroy on this and other
      # ephemeral workspace elsewhere in this workflow.
      #
      # HISTORICAL NOTE: I used retry action before
      # (https://github.com/nick-fields/retry) to execute apply
      # and destroy in, so we would unlock on failure and try the
      # apply or destroy again. The problem with that was it
      # was hiding all the output from the command line operations.
      # Also, I think the approach of unlocking, with continue-on-error
      # if there is an error because the workspace was not locked
      # to begin with, is neater.
      - name: Remove any existing lock on preview-ephemeral workspace
        # No problem if the workspace was not locked to begin with
        continue-on-error: true
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraform block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}
        # Contrary to what documentation for `terraform force-unlock` says
        # and what you would find on the internet, the Lock id for this
        # workspace is not a UUID nonce and the ID that is returned in
        # the error response that is supposed to work to unlock this
        # workspace does not work when passed as lock id to `terraform force-unlock`.
        # Instead, the lock id is of the form <org name>/<workspace name>
        run: |
          terraform force-unlock -force "${{ secrets.TF_ORG }}/${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}"
        working-directory: .iac/workspaces/preview-ephemeral
      - name: Terraform Apply
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraform block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}

          # these need to be in environment
          # when terraform apply runs and are
          # picked up by azurerm provider
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

          # This needs to be in environment when
          # terraform apply run and is picked up
          # Neon's Terraform provider
          NEON_API_KEY: ${{ secrets.NEON_API_KEY }}

          # This needs to be in environment when
          # terraform apply runs and is picked up
          # Vercel's Terraform provider
          VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}

          # This needs to be in environment when
          # terraform apply runs and is picked up
          # by GitHub's Terraform provider
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}

        # values of the variables, which are read from secrets and
        # environment variables of Preview environment in the GitHub
        # repo, are mostly the same as those decalared in Preview
        # workspace's Variables in Terraform HCP
        run: |
          terraform apply -auto-approve \
          -var="vault_name=${{ vars.VAULT_NAME }}" \
          -var="vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
          -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
          -var="secretname_for_psql_owner_connectionstring=${{ needs.compute-non-secret-vars.outputs.psql_owner_conn_name }}" \
          -var="environmentname_for_secrets_and_variables=Preview" \
          -var="repository_for_secrets_and_variables=flowmazondotnet" \
          -var="neon_project_id=${{ secrets.TF_MANAGED_NEON_PROJECT_ID }}" \
          -var="neon_source_branch_id=${{ secrets.TF_MANAGED_NEON_SOURCE_BRANCH_ID }}" \
          -var="neon_new_branch_name=${{ needs.compute-non-secret-vars.outputs.neon_branch_name }}" \
          -var="neon_database_name=${{ vars.NEON_DATABASE_NAME }}" \
          -var="neon_app_role=${{ vars.NEON_APP_ROLE }}" \
          -var="neon_owner_role=${{ vars.NEON_OWNER_ROLE }}" \
          -var="neon_app_role_password=${{ secrets.TF_MANAGED_NEON_APP_ROLE_PASSWORD }}" \
          -var="neon_owner_role_password=${{ secrets.TF_MANAGED_NEON_OWNER_ROLE_PASSWORD }}" \
          -var="vercel_team_id=${{ secrets.VERCEL_TEAM_ID }}" \
          -var="vercel_project_name=${{ needs.compute-non-secret-vars.outputs.vercel_project_name }}" \
          -var="vercel_app_domain_name=${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
          -var="vercel_region_for_server_side_execution=${{ vars.VERCEL_REGION_FOR_SERVER_SIDE_EXECUTION }}" \
          -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
          -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
          -var="grafanacloud_stack_slug=${{ vars.GRAFANA_STACK_SLUG }}" \
          -var="grafanacloud_frontend_o11y_api_access_token=${{ secrets.GRAFANA_FRONTEND_O11Y_API_ACCESS_TOKEN }}" \
          -var="grafanacloud_frontend_o11y_instance_name=${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}" \
          -var="secretname_for_grafanacloud_frontend_o11y_instance_url=${{ needs.compute-non-secret-vars.outputs.grafana_instance_url_secret }}" \
          -var="env_NEXT_PUBLIC_OTEL_ENVIRONMENT=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
          -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
          -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
          -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
          -var="secretname_for_vercel_project_id=${{ needs.compute-non-secret-vars.outputs.vercel_project_id_secret }}"
        working-directory: .iac/workspaces/preview-ephemeral

  migrate-db:
    environment:
      name: Preview
    needs: [create-update-preview-ephemeral, compute-non-secret-vars]
    name: Migrate Database
    runs-on: ubuntu-latest
    if: github.event.action != 'closed'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install psql client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run migrations
        env:
          MIGRATIONS_DIR: ./flowmazonbackend/flowmazonapi/MigrationScripts
        run: |
          for f in "${{ env.MIGRATIONS_DIR }}"/*.sql; do
            filename=$(basename "${f}")
            echo "Applying migration ${filename}"
            psql "${{ secrets[needs.compute-non-secret-vars.outputs.psql_owner_conn_name] }}" -f "${f}"
          done
        # Create infrastructure specific to a preview environment

  deploy-backend:
    environment:
      name: Preview
    permissions:
      contents: read # Needed for actions/checkout
      packages: write # Needed to push to GHCR
    # even though it needs db to be available, this
    # task takes so long to deploy (Azure every time! :( )
    # that I have removed its dependency on migrate-db
    # which is ok as this is only a Preview env
    needs: [compute-non-secret-vars, create-update-preview-ephemeral]
    name: Deploy API to Azure
    runs-on: ubuntu-latest
    if: github.event.action != 'closed'
    env:
      PR_LABEL: ${{ needs.compute-non-secret-vars.outputs.pr_label }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Print version to console
        run: |
          echo "Version number of the code artifacts produced for ths pull request-specific Preview environment would be: ${{ needs.compute-non-secret-vars.outputs.new_version }}"
      - name: Login to Azure
        uses: azure/login@v1
        with:
          # The referenced secret AZURE_CREDENTIALS contains
          # JSON with client id, client secret, tenant id and
          # subscription id of the service principal which
          # has AcrPush permission on the subscrpition or at least
          # on the ACR instance to which it will push in this job
          # within the subscription whose subscription id is provided.
          #
          # Format of the JSON is:
          # {
          #     "clientId": "<visible on service principal when you create it in portal>",
          #     "clientSecret": "<in portal you have to create it after creating the service principal>",
          #     "subscriptionId": "<subscription id of the subscription on which you have given AcrPush permission to the service principal>",
          #     "tenantId": "<visible on service principal when you create it in portal>"
          # }
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }} # Auto-generated, no setup needed!

      - name: Build and Push Docker image
        run: |
          cd ./flowmazonbackend
          FULL_IMAGE_NAME=ghcr.io/${{ vars.IMAGE_REPOSITORY }}:${{ needs.compute-non-secret-vars.outputs.new_version }}
          docker build -t ${FULL_IMAGE_NAME} .
          docker push ${FULL_IMAGE_NAME}

      # Like preview-ephemeral workspace created/synced above,
      # preview-ephemeral-api HCP workspace would be created
      # dynamically by this workflow, then deployed to using
      # local execution while the state file is maintained in
      # HCP.
      # The choices made for the next few steps are also
      # very similar to that job so see documentation above
      # for the create-update-preview-ephemeral job.

      # Hence, to create preview-ephemeral-api workspace,
      # we proceed as follows:
      - name: Create or update preview-ephemeral-api HCP workspace
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_PROJECT: ${{ secrets.TF_PROJECT }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          PR_LABEL: ${{ needs.compute-non-secret-vars.outputs.pr_label }}
          TF_API_WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          curl -sS \
            -H "Authorization: Bearer ${{env.HCP_TF_TOKEN}}" \
            -H "Content-Type: application/vnd.api+json" \
            -X POST \
            -d '{
              "data": {
                "attributes": {
                  "name": "'"${{ env.TF_API_WORKSPACE_NAME }}"'",
                  "auto-apply": false,
                  "execution-mode": "local"
                  
                },
                "type": "workspaces",
                "relationships": {
                  "tag-bindings": {
                    "data": [
                      {
                        "type": "tag-bindings",
                        "attributes": {
                          "key": "preview-ephemeral-api",
                          "value": ""
                        }
                      }
                    ]
                  },
                  "project": {
                    "data": {
                      "id": "'"${{ env.HCP_TF_PROJECT }}"'",
                      "type": "projects"
                    }
                  }
                }
              }
            }' \
            "https://app.terraform.io/api/v2/organizations/${{ env.HCP_TF_ORG }}/workspaces" \
            | tee response.json

          # If workspace already exists, the API will return an error. Ignore if already exists.
          if grep -q '"errors"' response.json; then
            if grep -q 'has already been taken' response.json; then
              echo "Workspace ${{ env.TF_API_WORKSPACE_NAME }} already exists (thath's great!). Continuing to next step..."
            else
              cat response.json
              exit 1
            fi
          fi
      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          # cli_config_credentials_hostname parameter default
          # to app.terraform.io which is what we want
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral-api
      # For reasoning behind, and explantion of, terraform force-unlock
      # command in the action below, see create-update-preview-ephemeral job
      - name: Remove any existing lock on preview-ephemeral-api workspace
        continue-on-error: true
        env:
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_api_workspace_name}}
        run: |
          terraform force-unlock -force "${{ secrets.TF_ORG }}/${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}"
        working-directory: .iac/workspaces/preview-ephemeral-api
      - name: Terraform Apply
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}

          # these need to be in environment
          # when terraform apply runs and are
          # picked up by azurerm provider
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

        # values of the variables, which are read from secrets and
        # environment variables of Preview environment in the GitHub
        # repo, are mostly the same as those decalared in Preview
        # workspace's Variables in Terraform HCP.
        run: |
          terraform apply -auto-approve \
          -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
          -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
          -var="app_resource_group_name=${{ needs.compute-non-secret-vars.outputs.api_resource_group_name }}" \
          -var="app_resource_group_location=${{ vars.API_RESOURCE_GROUP_LOCATION }}" \
          -var="app_name=${{ needs.compute-non-secret-vars.outputs.api_app_name }}" \
          -var="app_environment_name=${{ needs.compute-non-secret-vars.outputs.api_app_environment_name }}" \
          -var="app_domain_name=${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}" \
          -var="app_container_name=${{ vars.API_CONTAINER_NAME }}" \
          -var="app_container_port=${{ vars.API_CONTAINER_PORT }}" \
          -var="github_organisation_or_account=${{ github.actor }}" \
          -var="github_token_for_registry_read=${{ secrets.API_APP_GH_TOKEN }}" \
          -var="vault_secretname_registry_password_or_token=${{ needs.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}" \
          -var="image_repository=${{ vars.IMAGE_REPOSITORY }}" \
          -var="image_tag=${{ needs.compute-non-secret-vars.outputs.new_version }}" \
          -var="managed_identity_name=${{ vars.MANAGED_IDENTITY_NAME }}" \
          -var="id_and_vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
          -var="key_vault_name=${{ vars.VAULT_NAME }}" \
          -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
          -var="allowed_cors_origins_for_api=https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
          -var="env_OTEL_RESOURCE_ATTRIBUTES=deployment.environment.name=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
          -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
          -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
          -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
          -var="vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ needs.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}"
        working-directory: .iac/workspaces/preview-ephemeral-api

  standalone-tests:
    uses: "./.github/workflows/standalone-tests.yml"
    needs: [compute-non-secret-vars]
    environment:
      name: Preview
    secrets:
      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}
  playwright-tests:
    name: Playwright Integration Tests
    runs-on: ubuntu-24.04
    needs:
      [
        compute-non-secret-vars,
        deploy-frontend-to-vercel,
        deploy-backend,
        migrate-db,
      ]
    if: github.event.action != 'closed'
    environment:
      name: Preview
    env:
      BASE_URL: https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 24.x
      - name: Install dependencies and browsers
        working-directory: ./flowmazonfrontend
        run: |
          npm ci
          npx playwright install --with-deps

      # # This job is needed because the ACA app, if not already
      # # running, can take many minutes to come online where
      # # Playwright wait for 30 sec by default.
      # # Using this action instead of increasing timeout allows
      # # us to remove the dependency of this job on deploy-backend
      # # task in Preview environment were we have more flexibility.
      # # The reason for wanting to remove this dependency is
      # # that installing Playwright browsers can take many minutes
      # # and this can be parallelised with everything else in the
      # # the CI pipeline (which deployes to Preview environment
      # # for the pull request).
      # - name: Wait for API
      #   uses: emilioschepis/wait-for-endpoint@v1.0.3
      #   with:
      #     url: https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}/health/ready
      #     method: GET
      #     expected-status: 200
      #     timeout: 300000 # 5 minutes
      #     interval: 1000

      # # since we are going to have to wait for the API anyway,
      # # regardless of whether we put deploy-backend task as dependency
      # # of this ask or not (this is because, as mentioned above,
      # # ACA app can sometimes take very long time to start up),
      # # therefore we might as well remove dependency on deploy-frontend
      # # job and just wait for the frontend to come online.
      # # This allows us to start the current time consuming task
      # # to start even earlier in Preview.
      # - name: Wait for Frontend app
      #   uses: emilioschepis/wait-for-endpoint@v1.0.3
      #   with:
      #     url: https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}
      #     method: GET
      #     expected-status: 200
      #     timeout: 300000 # 5 minutes
      #     interval: 1000

      - name: Run Playwright tests
        working-directory: ./flowmazonfrontend
        # We want all three browsers for which projects have been defined in playwright.config.ts to run.
        # However we need to specify the projects explicitly as there are other projects for other purposes that are not specific to/are not meant to specify browsers to run tests on.
        run: npx playwright test --project chromium firefox webkit
      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report
          path: ./flowmazonfrontend/playwright-report/
          retention-days: 30
      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./flowmazonfrontend/junit-report/test_results.xml
      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Playwright Integration Tests
          path: ./flowmazonfrontend/junit-report/test_results.xml
          # java-junit reporter is experimental in this action. If this doesn't work properly then reporter should be jest-junit.
          # Bear in mind though that to use jest-junit, you need to surround the contents of this xml file with a <testsuites> element which has a `time` attribute. See my wiki for details.
          reporter: java-junit
          fail-on-error: false

  api-integration-tests:
    name: API Integration Tests
    runs-on: ubuntu-24.04
    needs: [compute-non-secret-vars]
    if: github.event.action != 'closed'
    environment:
      name: Preview
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 9.0.x
      - name: Restore dependencies
        working-directory: ./flowmazonbackend/flowmazonapi.IntegrationTests
        run: dotnet restore
      - name: Run Integration Tests
        working-directory: ./flowmazonbackend/flowmazonapi.IntegrationTests
        # Generating test results file in JUnit format (test_results.xml, not coverage report file which will not be generated as we are do not collect coverage from integration test runs as its usually not possible or straightforward to do that) because this is the only format CodeCov currently accepts test results in for its experimental test analytics feature (this is separate to its code coverage feature).
        # In order for dotnet test to be able to use test run report in this format, we need to add reference to `JunitXml.TestLogger` in the test project (not in the project under test), as described here:
        # https://github.com/spekt/junit.testlogger
        #
        # We also upload the test_results.xml file as a Check (which appears in the Checks tab of the pull request) using dorny/test-reporter action as the CodeCov Test Results Analytics feature is still experimental.
        run: |
          dotnet test --configuration Release \
            --logger "junit;LogFileName=test_results.xml" \
            --results-directory ./TestResults \
            --collect:"XPlat Code Coverage"
      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: API Integration Tests
          path: flowmazonbackend/flowmazonapi.IntegrationTests/TestResults/**/test_results.xml
          # java-junit reporter is experimental in this action. If this doesn't work properly then reporter should be jest-junit.
          # Bear in mind though that to use jest-junit, you need to surround the contents of this xml file with a <testsuites> element which has a `time` attribute. See my wiki for details.
          reporter: java-junit
          fail-on-error: false
      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: flowmazonbackend/flowmazonapi.IntegrationTests/TestResults/**/test_results.xml
  # This job is based on job of same name
  # in release.yml. See detailed comments
  # there.
  #
  # Also note that even though this is a Preview environment, we have created a separate Vercel project above for it (as part of ephemeral environment's infrastructure creation using Terraform). Every Vercel project, including this one for this ephemeral Preview environment, allows you to create a Production or a Staging deployment. We shall create a Production deployment, which is why, somewhat confusingly, `--prod` flag is usedi with `vercel build` and `vercel deploy` commands below even though this is a Preview deployment.
  deploy-frontend-to-vercel:
    name: Deploy Next.js frontend to Vercel
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: [compute-non-secret-vars, create-update-preview-ephemeral]
    if: github.event.action != 'closed'
    env:
      VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
      VERCEL_ORG_ID: ${{ secrets.VERCEL_TEAM_ID }}
      VERCEL_PROJECT_ID: ${{ secrets[needs.compute-non-secret-vars.outputs.vercel_project_id_secret] }}
    steps:
      - uses: actions/checkout@v4
      - name: Print version to console
        run: |
          echo "Version number for Preview deployment: ${{ needs.compute-non-secret-vars.outputs.new_version }}"
      - name: Update Version Number in package.json
        working-directory: ./flowmazonfrontend
        run: npm --no-git-tag-version version ${{ needs.compute-non-secret-vars.outputs.new_version }}
      - name: Install Vercel CLI
        working-directory: ./flowmazonfrontend
        run: npm install --global vercel@latest
      - name: Pull Vercel Environment Information
        working-directory: ./flowmazonfrontend
        run: vercel pull --yes --environment=preview --token=${{ secrets.VERCEL_API_TOKEN }}
      - name: Build Project Artifacts
        working-directory: ./flowmazonfrontend
        run: vercel build --prod --token=${{ secrets.VERCEL_API_TOKEN }}
        env:
          NEXT_PUBLIC_BACKEND_URL: https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}
          NEXT_PUBLIC_OTEL_ENVIRONMENT: ${{ needs.compute-non-secret-vars.outputs.otel_env }}
          NEXT_PUBLIC_FARO_URL: ${{ secrets[needs.compute-non-secret-vars.outputs.grafana_instance_url_secret] }}
          NEXT_PUBLIC_FARO_SERVICE_NAME: ${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}

      - name: Deploy Project Artifacts to Vercel
        working-directory: ./flowmazonfrontend
        id: deploy-artifacts
        run: |
          previewUrl=$(vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_API_TOKEN }})
          echo "previewUrl=$previewUrl" >> "$GITHUB_OUTPUT"

  chromatic:
    if: github.event.action != 'closed'
    uses: "./.github/workflows/chromatic.yml"
    with:
      failOnChanges: false
      workingDir: ./flowmazonfrontend
    secrets:
      CHROMATIC_PROJECT_TOKEN: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}

  # The job below gives us three things:
  #
  # 1. Runs interaction tests (only some stories have these).
  #
  # 2. Gives us coverage for all stories in the Storybook
  # (including those that do not have interaction tests).
  # Since the final render of every story (after interaction
  # test, if any, has run) get visual-tested in Chromatic
  # (these are in a sense asserts on the stories that are
  # not contained in the stories themselves),
  # coverage that includes stories both with and
  # without interaction tests gives makes sense and is
  # a complete coverage report for unit tests (stories) of
  # flowmazonfrontend app.
  #
  # 3. Because of the way storybook has been configured
  # (see .storybook/test-runner.js in flowmazonfrontend),
  # automated accessibility tests suite of axe-core
  # is run on the final render of every story.
  # Now individual visual states of a component
  # are highlighted in separate stories. In particular,
  # the initial (starting) state of a story with an
  # interaction tests - for this story accessibility tests
  # would run only on the final render - the starting
  # visual state would have a story of its own.
  # For this reason, and the fact that we write stories
  # all the way up to an entire screen/page (which would be
  # a component, most likely with behaviour) and it is only
  # behaviour across pages that is tested in Playwright tests,
  # the accessibility tests ru non stories are the only
  # automated accessibility tests we need for the frontend
  # app.
  # In particular, we DO NOT need to run axe-core in
  # Playwright tests also.
  #
  # THEREFORE WE NEED TO EXTRACT THREE THINGS IN THIS JOB:
  # 1. Coverage reported by Storybook test-runner. This
  # needs to be posted to CodeCov
  # 2. Test run report. This would combine both interaction
  # test results (where stories have them) and whether stories
  # rendered without errors (actual asserts on render
  # of a tory are Chromatic visual diffs and would happen in
  # Chromatic).
  # This needs to be posted to CodeCov as well as uploaded
  # as a check on the PR.
  # 3.Accessibility Test report. Q. WHERE SHOULD I UPLOAD THIS?
  storybook-tests:
    name: Storybook Tests
    runs-on: ubuntu-24.04
    if: github.event.action != 'closed'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 24.x
      - name: Install dependencies and playeright browsers
        working-directory: ./flowmazonfrontend
        run: |
          npm ci
          npx playwright install --with-deps

      - name: Build Storybook
        working-directory: ./flowmazonfrontend
        run: npm run build-storybook
      - name: Run Storybook tests
        working-directory: ./flowmazonfrontend
        run: npm run test-storybook:ci
      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Storybook Tests
          path: ./flowmazonfrontend/junit.xml # Path to test results
          reporter: jest-junit # Format of test results
          fail-on-error: false
      - name: Upload test results to Codecov
        if: ${{ !cancelled() }}
        uses: codecov/test-results-action@v1
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./flowmazonfrontend/junit.xml
      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v5
        with:
          files: ./flowmazonfrontend/sbcoverage/coverage-storybook.json
          fail_ci_if_error: true
          #flags: api-unittests
          name: storybook-tests
          token: ${{ secrets.CODECOV_TOKEN }}
          verbose: true

  comment-preview-links:
    name: Post links to Preview env on PR
    runs-on: ubuntu-24.04
    needs:
      [
        compute-non-secret-vars,
        deploy-backend,
        deploy-frontend-to-vercel,
        playwright-tests,
        chromatic,
      ]
    if: github.event.action != 'closed'
    steps:
      - name: Post sticky comment with preview links
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          recreate: true # so it always appears at the end where PR checks are
          header: preview-environment-links
          # recreate: true
          message: |
            # Preview Environment Links

            - **Web:** https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}

            - **API:** https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}

            - **Playwright Test Report:** download from **Artifacts section at the bottom of [this page](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})**

            ## Note

            - Links given above are for the last **successful** deployment to the Preview environment for this pull request. 

                If the latest deployment failed (see Checks section below) then these links could be out of date (i.e. could take you to a previous Preview deployment or may not work at all).

            -  Playwright HTML report would have screenshots, videos and interactive DOM snapshots for failed tests only.

            -  Summary test run reports are available for each test suite (API Unit Tests, API Integration Tests, Playwright Integration Tests) in the Checks section of this pull request. 

                They would be in a random section in Checks. Due to a limitation of the GitHub API, it is not possible to control where in Checks they appear.

  destroy-ephemeral-environment:
    name: Destroy Infrastructure for PR's Preview
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: compute-non-secret-vars
    if: github.event.action == 'closed'
    env:
      # Providers' env requirements
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      NEON_API_KEY: ${{ secrets.NEON_API_KEY }}
      VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
      GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral-api
      # For reasoning behind, and explantion of, terraform force-unlock
      # command in the action below, see create-update-preview-ephemeral job
      - name: Remove any existing lock on preview-ephemeral-api workspace
        continue-on-error: true
        env:
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_api_workspace_name}}
        run: |
          terraform force-unlock -force "${{ secrets.TF_ORG }}/${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}"
        working-directory: .iac/workspaces/preview-ephemeral-api
      - name: Destroy API infrastructure (preview-ephemeral-api)
        working-directory: .iac/workspaces/preview-ephemeral-api
        env:
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          terraform destroy -auto-approve \
            -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
            -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
            -var="app_resource_group_name=${{ needs.compute-non-secret-vars.outputs.api_resource_group_name }}" \
            -var="app_resource_group_location=${{ vars.API_RESOURCE_GROUP_LOCATION }}" \
            -var="app_name=${{ needs.compute-non-secret-vars.outputs.api_app_name }}" \
            -var="app_environment_name=${{ needs.compute-non-secret-vars.outputs.api_app_environment_name }}" \
            -var="app_domain_name=${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}" \
            -var="app_container_name=${{ vars.API_CONTAINER_NAME }}" \
            -var="app_container_port=${{ vars.API_CONTAINER_PORT }}" \
            -var="github_organisation_or_account=${{ github.actor }}" \
            -var="github_token_for_registry_read=${{ secrets.API_APP_GH_TOKEN }}" \
            -var="vault_secretname_registry_password_or_token=${{ needs.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}" \
            -var="image_repository=${{ vars.IMAGE_REPOSITORY }}" \
            -var="image_tag=${{ needs.compute-non-secret-vars.outputs.new_version }}" \
            -var="managed_identity_name=${{ vars.MANAGED_IDENTITY_NAME }}" \
            -var="id_and_vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
            -var="key_vault_name=${{ vars.VAULT_NAME }}" \
            -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
            -var="allowed_cors_origins_for_api=https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
            -var="env_OTEL_RESOURCE_ATTRIBUTES=deployment.environment.name=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
            -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
            -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
            -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
            -var="vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ needs.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}"
      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraform block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral
      # For reasoning behind, and explantion of, terraform force-unlock
      # command in the action below, see create-update-preview-ephemeral job
      - name: Remove any existing lock on preview-ephemeral workspace
        # No problem if the workspace was not locked to begin with
        continue-on-error: true
        env:
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}
        run: |
          terraform force-unlock -force "${{ secrets.TF_ORG }}/${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}"
        working-directory: .iac/workspaces/preview-ephemeral
      - name: Destroy preview-ephemeral infrastructure
        working-directory: .iac/workspaces/preview-ephemeral
        env:
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        run: |
          terraform destroy -auto-approve \
            -var="vault_name=${{ vars.VAULT_NAME }}" \
            -var="vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
            -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
            -var="secretname_for_psql_owner_connectionstring=${{ needs.compute-non-secret-vars.outputs.psql_owner_conn_name }}" \
            -var="environmentname_for_secrets_and_variables=Preview" \
            -var="repository_for_secrets_and_variables=flowmazondotnet" \
            -var="neon_project_id=${{ secrets.TF_MANAGED_NEON_PROJECT_ID }}" \
            -var="neon_source_branch_id=${{ secrets.TF_MANAGED_NEON_SOURCE_BRANCH_ID }}" \
            -var="neon_new_branch_name=${{ needs.compute-non-secret-vars.outputs.neon_branch_name }}" \
            -var="neon_database_name=${{ vars.NEON_DATABASE_NAME }}" \
            -var="neon_app_role=${{ vars.NEON_APP_ROLE }}" \
            -var="neon_owner_role=${{ vars.NEON_OWNER_ROLE }}" \
            -var="neon_app_role_password=${{ secrets.TF_MANAGED_NEON_APP_ROLE_PASSWORD }}" \
            -var="neon_owner_role_password=${{ secrets.TF_MANAGED_NEON_OWNER_ROLE_PASSWORD }}" \
            -var="vercel_team_id=${{ secrets.VERCEL_TEAM_ID }}" \
            -var="vercel_project_name=${{ needs.compute-non-secret-vars.outputs.vercel_project_name }}" \
            -var="vercel_app_domain_name=${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
            -var="vercel_region_for_server_side_execution=${{ vars.VERCEL_REGION_FOR_SERVER_SIDE_EXECUTION }}" \
            -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
            -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
            -var="grafanacloud_stack_slug=${{ vars.GRAFANA_STACK_SLUG }}" \
            -var="grafanacloud_frontend_o11y_api_access_token=${{ secrets.GRAFANA_FRONTEND_O11Y_API_ACCESS_TOKEN }}" \
            -var="grafanacloud_frontend_o11y_instance_name=${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}" \
            -var="secretname_for_grafanacloud_frontend_o11y_instance_url=${{ needs.compute-non-secret-vars.outputs.grafana_instance_url_secret }}" \
            -var="env_NEXT_PUBLIC_OTEL_ENVIRONMENT=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
            -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
            -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
            -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
            -var="secretname_for_vercel_project_id=${{ needs.compute-non-secret-vars.outputs.vercel_project_id_secret }}"

      - name: Install jq for JSON parsing
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Delete HCP Terraform workspace (preview-ephemeral-api)
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          echo "Deleting HCP Terraform workspace: ${WORKSPACE_NAME}"
          # Get workspace by name to obtain its ID
          response=$(curl -s -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            "https://app.terraform.io/api/v2/organizations/${HCP_TF_ORG}/workspaces/${WORKSPACE_NAME}" || echo '{"errors":[]}')

          if echo "$response" | jq -e '.errors' >/dev/null 2>&1; then
            echo "Workspace ${WORKSPACE_NAME} not found or already deleted. Skipping."
            exit 0
          fi

          workspace_id=$(echo "$response" | jq -r '.data.id')
          if [ -z "$workspace_id" ] || [ "$workspace_id" = "null" ]; then
            echo "Could not resolve workspace ID for ${WORKSPACE_NAME}. Skipping."
            exit 0
          fi

          # Delete workspace by ID
          delete_response=$(curl -s -w "%{http_code}" -o /dev/null \
            -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            -X DELETE "https://app.terraform.io/api/v2/workspaces/${workspace_id}")

          if [ "$delete_response" = "200" ] || [ "$delete_response" = "204" ] || [ "$delete_response" = "404" ]; then
            echo "Successfully deleted workspace ${WORKSPACE_NAME} (ID: ${workspace_id})"
          else
            echo "Failed to delete workspace ${WORKSPACE_NAME} (HTTP ${delete_response})"
            exit 1
          fi

      - name: Delete HCP Terraform workspace (preview-ephemeral)
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        run: |
          echo "Deleting HCP Terraform workspace: ${WORKSPACE_NAME}"
          # Get workspace by name to obtain its ID
          response=$(curl -s -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            "https://app.terraform.io/api/v2/organizations/${HCP_TF_ORG}/workspaces/${WORKSPACE_NAME}" || echo '{"errors":[]}')

          if echo "$response" | jq -e '.errors' >/dev/null 2>&1; then
            echo "Workspace ${WORKSPACE_NAME} not found or already deleted. Skipping."
            exit 0
          fi

          workspace_id=$(echo "$response" | jq -r '.data.id')
          if [ -z "$workspace_id" ] || [ "$workspace_id" = "null" ]; then
            echo "Could not resolve workspace ID for ${WORKSPACE_NAME}. Skipping."
            exit 0
          fi

          # Delete workspace by ID
          delete_response=$(curl -s -w "%{http_code}" -o /dev/null \
            -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            -X DELETE "https://app.terraform.io/api/v2/workspaces/${workspace_id}")

          if [ "$delete_response" = "200" ] || [ "$delete_response" = "204" ] || [ "$delete_response" = "404" ]; then
            echo "Successfully deleted workspace ${WORKSPACE_NAME} (ID: ${workspace_id})"
          else
            echo "Failed to delete workspace ${WORKSPACE_NAME} (HTTP ${delete_response})"
            exit 1
          fi
