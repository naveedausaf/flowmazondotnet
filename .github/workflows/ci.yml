name: CI
# This concurrency block definition ensures that
# only a single instance of this workflow can be
# be running at any given time for a given
# source (i.e. head) branch
concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true
permissions:
  checks: write
  pull-requests: write
on:
  pull_request:
    branches:
      - main
    types: [opened, synchronize, reopened, closed]
jobs:
  compute-non-secret-vars:
    name: Compute non-secret variables for later jobs
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    outputs:
      pr_label: ${{ steps.compute-non-secret-vars.outputs.pr_label }}
      new_version: ${{ steps.compute-non-secret-vars.outputs.new_version }}
      tf_workspace_name: ${{ steps.compute-non-secret-vars.outputs.tf_workspace_name }}
      tf_api_workspace_name: ${{ steps.compute-non-secret-vars.outputs.tf_api_workspace_name }}
      vault_secret_conn_name: ${{ steps.compute-non-secret-vars.outputs.vault_secret_conn_name }}
      psql_owner_conn_name: ${{ steps.compute-non-secret-vars.outputs.psql_owner_conn_name }}
      neon_branch_name: ${{ steps.compute-non-secret-vars.outputs.neon_branch_name }}
      vercel_project_name: ${{ steps.compute-non-secret-vars.outputs.vercel_project_name }}
      vercel_app_domain: ${{ steps.compute-non-secret-vars.outputs.vercel_app_domain }}
      grafana_instance_name: ${{ steps.compute-non-secret-vars.outputs.grafana_instance_name }}
      grafana_instance_url_secret: ${{ steps.compute-non-secret-vars.outputs.grafana_instance_url_secret }}
      otel_env: ${{ steps.compute-non-secret-vars.outputs.otel_env }}
      vercel_project_id_secret: ${{ steps.compute-non-secret-vars.outputs.vercel_project_id_secret }}
      api_resource_group_name: ${{ steps.compute-non-secret-vars.outputs.api_resource_group_name }}
      api_app_name: ${{ steps.compute-non-secret-vars.outputs.api_app_name }}
      api_app_environment_name: ${{ steps.compute-non-secret-vars.outputs.api_app_environment_name }}
      api_app_domain_name: ${{ steps.compute-non-secret-vars.outputs.api_app_domain_name }}
      vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS: ${{ steps.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}
      vault_secretname_registry_password_or_token: ${{ steps.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}
    steps:
      - name: Compute non-secret variables for this and later jobs
        id: compute-non-secret-vars
        # This step writes out those vars that need to be available
        # later in this job or in other jobs in the workflow.
        #
        # Many of these would be put in env block
        # at the top (outside of all jobs) but since some
        # of them require github context to compute the value,
        # I needed to put them within a job (in this job, as it's
        # the first one on the chaon of build and deploy jobs)
        # and output to $GITHUB_OUTPUT, then further output those
        # from outputs block from the job. This ensures the
        # computed values are available to other jobs.
        #
        # Since I am collecting such vars/computed values
        # here in this step, even the ones that don't need to
        # be output for other jobs and only need to be used
        # by other steps of this very job, and which could
        # therefore be put in `env` block of this job,
        # have been collected here. So these would now
        # need to be referenced as
        # ${{ steps.compute-non-secret-vars.outputs.<var_name> }} from
        # other steps of this job rather than as
        # ${{ env.<var_name> }} in those steps.
        # CRUCIAL: All computed values should be non-secret
        # and based on secrets GitHub context. This makes them safe to (relatively) freely
        # share with any job in the workflow.
        env:
          # putting this in env block so I can access it in step
          # using ${{ env.PR_LABEL }} rather than ${ PR_LABEL}.
          # Using env context enabled me to check if env var
          # values are as they should be if I turn on
          # debug logging (which would show a missing
          # env var as being `null`).
          PR_LABEL: pr${{ github.event.number }}
        run: |
          echo "pr_label=${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          # we don't know what the version number would be when,
          # and if, this pull request is merged to main.
          # so best to use the place holder 0.0.0 that we also
          # use in package.json in NPM packages (which says
          # 0.0.0-semanticrelease in its "version" field;
          # I do the same in .NET csproj file's <version>
          # element also).
          # Of course we want to suffix 0.0.0 with the PR
          # label for the artifacts produced by this workflow
          # which runs on the source branch of a pull request.
          # NEW_VERSION: 
          echo "new_version=0.0.0-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "tf_workspace_name=preview-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "tf_api_workspace_name=preview-api-${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vault_secret_conn_name=${{ vars.VAULT_SECRETNAME_FOR_CONNECTIONSTRING_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "psql_owner_conn_name=${{ vars.SECRETNAME_FOR_PSQL_OWNER_CONNECTIONSTRING_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "neon_branch_name=${{ vars.NEON_NEW_BRANCH_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vercel_project_name=${{ env.PR_LABEL }}${{ vars.VERCEL_PROJECT_NAME_SUFFIX }}" >> $GITHUB_OUTPUT

          echo "vercel_app_domain=${{ env.PR_LABEL }}.${{ vars.VERCEL_APP_APEX_DOMAIN_NAME }}" >> $GITHUB_OUTPUT

          echo "grafana_instance_name=${{ vars.NEXT_PUBLIC_FARO_SERVICE_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "grafana_instance_url_secret=${{ vars.SECRETNAME_FOR_GRAFANACLOUD_FRONTEND_O11Y_INSTANCE_URL_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "otel_env=${{ vars.OTEL_ENVIRONMENT_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "vercel_project_id_secret=${{ vars.SECRETNAME_FOR_VERCEL_PROJECT_ID_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_resource_group_name=${{ vars.API_RESOURCE_GROUP_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_name=${{ vars.API_APP_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_environment_name=${{ vars.API_APP_ENVIRONMENT_NAME_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          echo "api_app_domain_name=api-${{ env.PR_LABEL }}.${{ vars.VERCEL_APP_APEX_DOMAIN_NAME }}" >> $GITHUB_OUTPUT

          echo "vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ vars.VAULT_SECRETNAME_ENV_OTEL_EXPORTER_OTLP_HEADERS_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

          # The name of the secret in key vault in which GitHub PAT
          # or other token/secret for accessing the Container registry
          #would be sore
          echo "vault_secretname_registry_password_or_token=${{ vars.VAULT_SECRETNAME_REGISTRY_PASSWORD_OR_TOKEN_PREFIX }}${{ env.PR_LABEL }}" >> $GITHUB_OUTPUT

  audit-signatures:
    name: Audit signatures of NPM dependencies
    runs-on: ubuntu-24.04
    if: github.event.action != 'closed'
    steps:
      - uses: actions/checkout@v4
      - name: Audit Provenance Attestations and Signatures
        run: |
          npm ci
          npm audit signatures

  create-update-preview-ephemeral:
    name: Create or Update Infrastructure for PR's Preview
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: compute-non-secret-vars
    if: github.event.action != 'closed'
    steps:
      - uses: actions/checkout@v4

      - name: Create or update preview-ephemeral HCP workspace
        # We don't care if the worksapce already exists as
        # in that case this HCP API Would return an error
        # that we ignore so the step would do nothing.
        # This behaviour is useful if for whatever reason
        # the first time this workflow ran for a PR, the
        # workspace failed to be created. In that we can
        # re-run the workflow and it will try to create
        # the workspace again.
        # HENCE commenting out the line below:
        # if: github.event.action == 'opened'
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_PROJECT: ${{ secrets.TF_PROJECT }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          TF_WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        # It is CRUCIAL to set "execution-mode": "local"
        # in the "data" object in the JSON request body
        # in the cURL to create the TF workspace below.
        #
        # This ensures that terraform apply executes
        # locally even though the state file is maintained
        # in HCP Terraform.
        #
        # Without this, it would run on HCP's runner.
        # There it would fail on terraform apply (but
        # not on terraform init) with the message that modules
        # referenced using `../../modules/<module folder>`
        # could not be read:
        #
        # │ Error: Unreadable module directory
        # │
        # │ Unable to evaluate directory symlink: lstat ../../modules: no such file or
        # │ directory
        # ╵
        # ╷
        # │ Error: Unreadable module directory
        # │
        # │ The directory  could not be read for module "flowmazonfrontend" at
        # │ main.tf:1.
        # ╵
        # Operation failed: failed running terraform init (exit 1)
        # Error: Terraform exited with code 1.
        #
        #
        # You can try to fix that by using git references
        # for the two modules used by the preview-ephemeral
        # root module that corresponds to the workspace:
        #
        # module "flowmazonfrontend" {
        #   source = "git::https://github.com/EnableHub/flowmazondotnet.git//.iac/modules/flowmazonfrontend?ref=main"
        #
        # and
        #
        # module "db_branch" {
        # source = "git::https://github.com/EnableHub/flowmazondotnet.git//.iac/modules/db_branch?ref=main"
        #
        # Then you would get an error if there are more than
        # one modules being referenced via github references
        # in the root module (a single module referenced like
        # this is fine though, which is a bit bafffling):
        #
        # ╷
        # │ Error: Failed to expand subdir globs
        # │
        # │ subdir ".iac/modules/flowmazonfrontend" not found
        #
        # Basically one of the modules cannot be deferennced.
        #
        # I did have the option of copying across the referenced
        # modules into the dir of the root module, then
        # modify the root module to change the references
        # dynamiclly in GitHub Actions workflow. This
        # would likely have worked and would have required
        # only local relative path references because the
        # moule folders would be within the root module's
        # folder.
        #
        # However, I have instead chosen to execute the TF
        # config locally while keeping the state file on
        # HCP Terraform. This is acceptable as the
        # GitHub Actions runner on which terraform apply
        # runs is also a clean machine and the state is
        # still persisted in and managed by HCP Terraform.

        run: |
          curl -sS \
            -H "Authorization: Bearer ${{ env.HCP_TF_TOKEN }}" \
            -H "Content-Type: application/vnd.api+json" \
            -X POST \
            -d '{
              "data": {
          "attributes": {
            "name": "'"${{ env.TF_WORKSPACE_NAME }}"'",
            "auto-apply": false,
            "execution-mode": "local"
            
          },
          "type": "workspaces",
          "relationships": {
            "tag-bindings": {
              "data": [
                {
            "type": "tag-bindings",
            "attributes": {
              "key": "preview-ephemeral",
              "value": ""
            }
                }
              ]
            },
            "project": {
              "data": {
                "id": "'"${{ env.HCP_TF_PROJECT }}"'",
                "type": "projects"
              }
            }
          }
              }
            }' \
            "https://app.terraform.io/api/v2/organizations/${{ env.HCP_TF_ORG }}/workspaces" \
            | tee response.json

          # If workspace already exists, the API will return an error. Ignore if already exists.
          if grep -q '"errors"' response.json; then
            if grep -q 'has already been taken' response.json; then
              echo "Workspace ${{ env.TF_WORKSPACE_NAME }} already exists (that's great!). Continuing to next step..."
            else
              cat response.json
              exit 1
            fi
          fi
      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraform block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral

      - name: Terraform Apply
        uses: nick-fields/retry@ce71cc2ab81d554ebbe88c79ab5975992d79ba08 # v3.0.2
        with:
          timeout_minutes: 30
          max_attempts: 2

          # Run terraform apply and capture lock ID on failure
          command: |
            cd .iac/workspaces/preview-ephemeral
            output=$(terraform apply -auto-approve \
              -var="vault_name=${{ vars.VAULT_NAME }}" \
              -var="vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
              -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
              -var="secretname_for_psql_owner_connectionstring=${{ needs.compute-non-secret-vars.outputs.psql_owner_conn_name }}" \
              -var="environmentname_for_secrets_and_variables=Preview" \
              -var="repository_for_secrets_and_variables=flowmazondotnet" \
              -var="neon_project_id=${{ secrets.TF_MANAGED_NEON_PROJECT_ID }}" \
              -var="neon_source_branch_id=${{ secrets.TF_MANAGED_NEON_SOURCE_BRANCH_ID }}" \
              -var="neon_new_branch_name=${{ needs.compute-non-secret-vars.outputs.neon_branch_name }}" \
              -var="neon_database_name=${{ vars.NEON_DATABASE_NAME }}" \
              -var="neon_app_role=${{ vars.NEON_APP_ROLE }}" \
              -var="neon_owner_role=${{ vars.NEON_OWNER_ROLE }}" \
              -var="neon_app_role_password=${{ secrets.TF_MANAGED_NEON_APP_ROLE_PASSWORD }}" \
              -var="neon_owner_role_password=${{ secrets.TF_MANAGED_NEON_OWNER_ROLE_PASSWORD }}" \
              -var="vercel_team_id=${{ secrets.VERCEL_TEAM_ID }}" \
              -var="vercel_project_name=${{ needs.compute-non-secret-vars.outputs.vercel_project_name }}" \
              -var="vercel_app_domain_name=${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
              -var="vercel_region_for_server_side_execution=${{ vars.VERCEL_REGION_FOR_SERVER_SIDE_EXECUTION }}" \
              -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
              -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
              -var="grafanacloud_stack_slug=${{ vars.GRAFANA_STACK_SLUG }}" \
              -var="grafanacloud_frontend_o11y_api_access_token=${{ secrets.GRAFANA_FRONTEND_O11Y_API_ACCESS_TOKEN }}" \
              -var="grafanacloud_frontend_o11y_instance_name=${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}" \
              -var="secretname_for_grafanacloud_frontend_o11y_instance_url=${{ needs.compute-non-secret-vars.outputs.grafana_instance_url_secret }}" \
              -var="env_NEXT_PUBLIC_OTEL_ENVIRONMENT=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
              -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
              -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
              -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
              -var="secretname_for_vercel_project_id=${{ needs.compute-non-secret-vars.outputs.vercel_project_id_secret }}" \
              2>&1)
            exit_code=$?
            if [ $exit_code -ne 0 ]; then
              lock_id=$(echo "$output" | grep -o 'ID: [0-9a-f-]\+' | cut -d' ' -f2)
              if [ -n "$lock_id" ]; then
                terraform force-unlock -force "$lock_id"
              fi
              exit $exit_code
            fi
          shell: bash
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}

          # these need to be in environment
          # when terraform apply runs and are
          # picked up by azurerm provider
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

          # This needs to be in environment when
          # terraform apply run and is picked up
          # Neon's Terraform provider
          NEON_API_KEY: ${{ secrets.NEON_API_KEY }}

          # This needs to be in environment when
          # terraform apply runs and is picked up
          # Vercel's Terraform provider
          VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}

          # This needs to be in environment when
          # terraform apply runs and is picked up
          # by GitHub's Terraform provider
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
      # - name: Terraform Apply
      #   env:
      #     # terraform init expects to be given TF_WORKSPACE env var
      #     # because it is not part of terraorm block in the root module
      #     TF_WORKSPACE: ${{needs.compute-non-secret-vars.outputs.tf_workspace_name}}

      #     # these need to be in environment
      #     # when terraform apply runs and are
      #     # picked up by azurerm provider
      #     ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      #     ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      #     ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      #     ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

      #     # This needs to be in environment when
      #     # terraform apply run and is picked up
      #     # Neon's Terraform provider
      #     NEON_API_KEY: ${{ secrets.NEON_API_KEY }}

      #     # This needs to be in environment when
      #     # terraform apply runs and is picked up
      #     # Vercel's Terraform provider
      #     VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}

      #     # This needs to be in environment when
      #     # terraform apply runs and is picked up
      #     # by GitHub's Terraform provider
      #     GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}

      #   # values of the variables, which are read from secrets and
      #   # environment variables of Preview environment in the GitHub
      #   # repo, are mostly the same as those decalared in Preview
      #   # workspace's Variables in Terraform HCP
      #   run: |
      #     terraform apply -auto-approve \
      #     -var="vault_name=${{ vars.VAULT_NAME }}" \
      #     -var="vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
      #     -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
      #     -var="secretname_for_psql_owner_connectionstring=${{ needs.compute-non-secret-vars.outputs.psql_owner_conn_name }}" \
      #     -var="environmentname_for_secrets_and_variables=Preview" \
      #     -var="repository_for_secrets_and_variables=flowmazondotnet" \
      #     -var="neon_project_id=${{ secrets.TF_MANAGED_NEON_PROJECT_ID }}" \
      #     -var="neon_source_branch_id=${{ secrets.TF_MANAGED_NEON_SOURCE_BRANCH_ID }}" \
      #     -var="neon_new_branch_name=${{ needs.compute-non-secret-vars.outputs.neon_branch_name }}" \
      #     -var="neon_database_name=${{ vars.NEON_DATABASE_NAME }}" \
      #     -var="neon_app_role=${{ vars.NEON_APP_ROLE }}" \
      #     -var="neon_owner_role=${{ vars.NEON_OWNER_ROLE }}" \
      #     -var="neon_app_role_password=${{ secrets.TF_MANAGED_NEON_APP_ROLE_PASSWORD }}" \
      #     -var="neon_owner_role_password=${{ secrets.TF_MANAGED_NEON_OWNER_ROLE_PASSWORD }}" \
      #     -var="vercel_team_id=${{ secrets.VERCEL_TEAM_ID }}" \
      #     -var="vercel_project_name=${{ needs.compute-non-secret-vars.outputs.vercel_project_name }}" \
      #     -var="vercel_app_domain_name=${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
      #     -var="vercel_region_for_server_side_execution=${{ vars.VERCEL_REGION_FOR_SERVER_SIDE_EXECUTION }}" \
      #     -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
      #     -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
      #     -var="grafanacloud_stack_slug=${{ vars.GRAFANA_STACK_SLUG }}" \
      #     -var="grafanacloud_frontend_o11y_api_access_token=${{ secrets.GRAFANA_FRONTEND_O11Y_API_ACCESS_TOKEN }}" \
      #     -var="grafanacloud_frontend_o11y_instance_name=${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}" \
      #     -var="secretname_for_grafanacloud_frontend_o11y_instance_url=${{ needs.compute-non-secret-vars.outputs.grafana_instance_url_secret }}" \
      #     -var="env_NEXT_PUBLIC_OTEL_ENVIRONMENT=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
      #     -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
      #     -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
      #     -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
      #     -var="secretname_for_vercel_project_id=${{ needs.compute-non-secret-vars.outputs.vercel_project_id_secret }}"
      #   working-directory: .iac/workspaces/preview-ephemeral

  migrate-db:
    environment:
      name: Preview
    needs: [create-update-preview-ephemeral, compute-non-secret-vars]
    name: Migrate Database
    runs-on: ubuntu-latest
    if: github.event.action != 'closed'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install psql client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Run migrations
        env:
          MIGRATIONS_DIR: ./flowmazonbackend/flowmazonapi/MigrationScripts
        run: |
          for f in "${{ env.MIGRATIONS_DIR }}"/*.sql; do
            filename=$(basename "${f}")
            echo "Applying migration ${filename}"
            psql "${{ secrets[needs.compute-non-secret-vars.outputs.psql_owner_conn_name] }}" -f "${f}"
          done
        # Create infrastructure specific to a preview environment

  deploy-backend:
    environment:
      name: Preview
    permissions:
      contents: read # Needed for actions/checkout
      packages: write # Needed to push to GHCR
    # even though it needs db to be available, this
    # task takes so long to deploy (Azure every time! :( )
    # that I have removed its dependency on migrate-db
    # which is ok as this is only a Preview env
    needs: [compute-non-secret-vars, create-update-preview-ephemeral]
    name: Deploy API to Azure
    runs-on: ubuntu-latest
    if: github.event.action != 'closed'
    env:
      PR_LABEL: ${{ needs.compute-non-secret-vars.outputs.pr_label }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Print version to console
        run: |
          echo "Version number of the code artifacts produced for ths pull request-specific Preview environment would be: ${{ needs.compute-non-secret-vars.outputs.new_version }}"
      - name: Login to Azure
        uses: azure/login@v1
        with:
          # The referenced secret AZURE_CREDENTIALS contains
          # JSON with client id, client secret, tenant id and
          # subscription id of the service principal which
          # has AcrPush permission on the subscrpition or at least
          # on the ACR instance to which it will push in this job
          # within the subscription whose subscription id is provided.
          #
          # Format of the JSON is:
          # {
          #     "clientId": "<visible on service principal when you create it in portal>",
          #     "clientSecret": "<in portal you have to create it after creating the service principal>",
          #     "subscriptionId": "<subscription id of the subscription on which you have given AcrPush permission to the service principal>",
          #     "tenantId": "<visible on service principal when you create it in portal>"
          # }
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }} # Auto-generated, no setup needed!

      - name: Build and Push Docker image
        run: |
          cd ./flowmazonbackend
          FULL_IMAGE_NAME=ghcr.io/${{ vars.IMAGE_REPOSITORY }}:${{ needs.compute-non-secret-vars.outputs.new_version }}
          docker build -t ${FULL_IMAGE_NAME} .
          docker push ${FULL_IMAGE_NAME}

      # Like preview-ephemeral workspace created/synced above,
      # preview-ephemeral-api HCP workspace would be created
      # dynamically by this workflow, then deployed to using
      # local execution while the state file is maintained in
      # HCP.
      # The choices made for the next two steps are also
      # very similar to that job so see documentation above
      # for the create-update-preview-ephemeral job.

      # Hence, to create preview-ephemeral-api workspace,
      # we proceed as follows:
      - name: Create or update preview-ephemeral-api HCP workspace
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_PROJECT: ${{ secrets.TF_PROJECT }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          PR_LABEL: ${{ needs.compute-non-secret-vars.outputs.pr_label }}
          TF_API_WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          curl -sS \
            -H "Authorization: Bearer ${{env.HCP_TF_TOKEN}}" \
            -H "Content-Type: application/vnd.api+json" \
            -X POST \
            -d '{
              "data": {
                "attributes": {
                  "name": "'"${{ env.TF_API_WORKSPACE_NAME }}"'",
                  "auto-apply": false,
                  "execution-mode": "local"
                  
                },
                "type": "workspaces",
                "relationships": {
                  "tag-bindings": {
                    "data": [
                      {
                        "type": "tag-bindings",
                        "attributes": {
                          "key": "preview-ephemeral-api",
                          "value": ""
                        }
                      }
                    ]
                  },
                  "project": {
                    "data": {
                      "id": "'"${{ env.HCP_TF_PROJECT }}"'",
                      "type": "projects"
                    }
                  }
                }
              }
            }' \
            "https://app.terraform.io/api/v2/organizations/${{ env.HCP_TF_ORG }}/workspaces" \
            | tee response.json

          # If workspace already exists, the API will return an error. Ignore if already exists.
          if grep -q '"errors"' response.json; then
            if grep -q 'has already been taken' response.json; then
              echo "Workspace ${{ env.TF_API_WORKSPACE_NAME }} already exists (thath's great!). Continuing to next step..."
            else
              cat response.json
              exit 1
            fi
          fi
      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          # cli_config_credentials_hostname parameter default
          # to app.terraform.io which is what we want
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
      - name: Terraform Init
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: terraform init
        working-directory: .iac/workspaces/preview-ephemeral-api

      - name: Terraform Apply
        env:
          # terraform init expects to be given TF_WORKSPACE env var
          # because it is not part of terraorm block in the root module
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}

          # these need to be in environment
          # when terraform apply runs and are
          # picked up by azurerm provider
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

        # values of the variables, which are read from secrets and
        # environment variables of Preview environment in the GitHub
        # repo, are mostly the same as those decalared in Preview
        # workspace's Variables in Terraform HCP.
        run: |
          terraform apply -auto-approve \
          -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
          -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
          -var="app_resource_group_name=${{ needs.compute-non-secret-vars.outputs.api_resource_group_name }}" \
          -var="app_resource_group_location=${{ vars.API_RESOURCE_GROUP_LOCATION }}" \
          -var="app_name=${{ needs.compute-non-secret-vars.outputs.api_app_name }}" \
          -var="app_environment_name=${{ needs.compute-non-secret-vars.outputs.api_app_environment_name }}" \
          -var="app_domain_name=${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}" \
          -var="app_container_name=${{ vars.API_CONTAINER_NAME }}" \
          -var="app_container_port=${{ vars.API_CONTAINER_PORT }}" \
          -var="github_organisation_or_account=${{ github.actor }}" \
          -var="github_token_for_registry_read=${{ secrets.API_APP_GH_TOKEN }}" \
          -var="vault_secretname_registry_password_or_token=${{ needs.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}" \
          -var="image_repository=${{ vars.IMAGE_REPOSITORY }}" \
          -var="image_tag=${{ needs.compute-non-secret-vars.outputs.new_version }}" \
          -var="managed_identity_name=${{ vars.MANAGED_IDENTITY_NAME }}" \
          -var="id_and_vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
          -var="key_vault_name=${{ vars.VAULT_NAME }}" \
          -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
          -var="allowed_cors_origins_for_api=https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
          -var="env_OTEL_RESOURCE_ATTRIBUTES=deployment.environment.name=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
          -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
          -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
          -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
          -var="vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ needs.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}"
        working-directory: .iac/workspaces/preview-ephemeral-api

  api-unittests:
    name: API Unit Tests
    runs-on: ubuntu-24.04
    needs: [compute-non-secret-vars]
    if: github.event.action != 'closed'
    environment:
      name: Preview
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 9.0.x
      - name: Restore dependencies
        working-directory: ./flowmazonbackend/flowmazonapi.UnitTests
        run: dotnet restore
      - name: Run unit tests with coverage
        working-directory: ./flowmazonbackend/flowmazonapi.UnitTests
        # Generating test results file (test_results.xml, not the cobertura format coverage file which is separate) in JUnit format because this is the only format CodeCov currently accepts test results in for its experimental test analytics feature (this is separate to its code coverage feature).
        # In order for dotnet test to be able to use test run report in this format, we need to add reference to `JunitXml.TestLogger` in the test project (not in the project under test), as described here:
        # https://github.com/spekt/junit.testlogger
        #
        # We also upload the test_results.xml file as a Check (which appears in the Checks tab of the pull request) using dorny/test-reporter action as the CodeCov Test Results Analytics feature is still experimental.
        run: |
          dotnet test --configuration Release \
            --logger "junit;LogFileName=test_results.xml" \
            --results-directory ./TestResults \
            --collect:"XPlat Code Coverage"
      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: API Unit Tests
          path: flowmazonbackend/flowmazonapi.UnitTests/TestResults/**/test_results.xml
          # java-junit reporter is experimental in this action. If this doesn't work properly then reporter should be jest-junit.
          # Bear in mind though that to use jest-junit, you need to surround the contents of this xml file with a <testsuites> element which has a `time` attribute. See my wiki for details.
          reporter: java-junit
          fail-on-error: false

      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v5
        with:
          files: flowmazonbackend/flowmazonapi.UnitTests/TestResults/**/coverage.cobertura.xml
          fail_ci_if_error: true
          #flags: api-unittests
          name: api-unittests
          token: ${{ secrets.CODECOV_TOKEN }}
          verbose: true
  playwright-tests:
    name: Playwright Integration Tests
    runs-on: ubuntu-24.04
    needs: [
        compute-non-secret-vars,
        #deploy-frontend-to-vercel,
        #deploy-backend,
        migrate-db,
      ]
    if: github.event.action != 'closed'
    environment:
      name: Preview
    env:
      BASE_URL: https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: 24.x
      - name: Install dependencies and browsers
        working-directory: ./flowmazonfrontend
        run: |
          npm ci
          npx playwright install --with-deps

      # This job is needed because the ACA app, if not already
      # running, can take many minutes to come online where
      # Playwright wait for 30 sec by default.
      # Using this action instead of increasing timeout allows
      # us to remove the dependency of this job on deploy-backend
      # task in Preview environment were we have more flexibility.
      # The reason for wanting to remove this dependency is
      # that installing Playwright browsers can take many minutes
      # and this can be parallelised with everything else in the
      # the CI pipeline (which deployes to Preview environment
      # for the pull request)
      - name: Wait for API
        uses: emilioschepis/wait-for-endpoint@v1.0.3
        with:
          url: https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}/health/ready
          method: GET
          expected-status: 200
          timeout: 300000 # 5 minutes
          interval: 1000

      # since we are going to have to wait for the API anyway,
      # regardless of whether we put deploy-backend task as dependency
      # of this ask or not (this is because, as mentioned above,
      # ACA app can sometimes take very long time to start up),
      # therefore we might as well remove dependency on deploy-frontend
      # job and just wait for the frontend to come online.
      # This allows us to start the current time consuming task
      # to start even earlier in Preview.
      - name: Wait for Frontend app
        uses: emilioschepis/wait-for-endpoint@v1.0.3
        with:
          url: https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}
          method: GET
          expected-status: 200
          timeout: 300000 # 5 minutes
          interval: 1000

      - name: Run Playwright tests
        working-directory: ./flowmazonfrontend
        # We want all three browsers for which projects have been defined in playwright.config.ts to run.
        # However we need to specify the projects explicitly as there are other projects for other purposes that are not specific to/are not meant to specify browsers to run tests on.
        run: npx playwright test --project chromium firefox webkit
      - uses: actions/upload-artifact@v4
        if: ${{ !cancelled() }}
        with:
          name: playwright-report
          path: ./flowmazonfrontend/playwright-report/
          retention-days: 30

      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: Playwright Integration Tests
          path: ./flowmazonfrontend/junit-report/test_results.xml
          # java-junit reporter is experimental in this action. If this doesn't work properly then reporter should be jest-junit.
          # Bear in mind though that to use jest-junit, you need to surround the contents of this xml file with a <testsuites> element which has a `time` attribute. See my wiki for details.
          reporter: java-junit
          fail-on-error: false

  api-integration-tests:
    name: API Integration Tests
    runs-on: ubuntu-24.04
    needs: [compute-non-secret-vars]
    if: github.event.action != 'closed'
    environment:
      name: Preview
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Setup .NET SDK
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 9.0.x
      - name: Restore dependencies
        working-directory: ./flowmazonbackend/flowmazonapi.IntegrationTests
        run: dotnet restore
      - name: Run Integration Tests
        working-directory: ./flowmazonbackend/flowmazonapi.IntegrationTests
        # Generating test results file in JUnit format (test_results.xml, not coverage report file which will not be generated as we are do not collect coverage from integration test runs as its usually not possible or straightforward to do that) because this is the only format CodeCov currently accepts test results in for its experimental test analytics feature (this is separate to its code coverage feature).
        # In order for dotnet test to be able to use test run report in this format, we need to add reference to `JunitXml.TestLogger` in the test project (not in the project under test), as described here:
        # https://github.com/spekt/junit.testlogger
        #
        # We also upload the test_results.xml file as a Check (which appears in the Checks tab of the pull request) using dorny/test-reporter action as the CodeCov Test Results Analytics feature is still experimental.
        run: |
          dotnet test --configuration Release \
            --logger "junit;LogFileName=test_results.xml" \
            --results-directory ./TestResults \
            --collect:"XPlat Code Coverage"
      - name: Publish test report to PR Checks
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: API Integration Tests
          path: flowmazonbackend/flowmazonapi.IntegrationTests/TestResults/**/test_results.xml
          # java-junit reporter is experimental in this action. If this doesn't work properly then reporter should be jest-junit.
          # Bear in mind though that to use jest-junit, you need to surround the contents of this xml file with a <testsuites> element which has a `time` attribute. See my wiki for details.
          reporter: java-junit
          fail-on-error: false

  # This job is based on job of same name
  # in release.yml. See detailed comments
  # there.
  #
  # Also note that even though this is a Preview environment, we have created a separate Vercel project above for it (as part of ephemeral environment's infrastructure creation using Terraform). Every Vercel project, including this one for this ephemeral Preview environment, allows you to create a Production or a Staging deployment. We shall create a Production deployment, which is why, somewhat confusingly, `--prod` flag is usedi with `vercel build` and `vercel deploy` commands below even though this is a Preview deployment.
  deploy-frontend-to-vercel:
    name: Deploy Next.js frontend to Vercel
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: [compute-non-secret-vars, create-update-preview-ephemeral]
    if: github.event.action != 'closed'
    env:
      VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
      VERCEL_ORG_ID: ${{ secrets.VERCEL_TEAM_ID }}
      VERCEL_PROJECT_ID: ${{ secrets[needs.compute-non-secret-vars.outputs.vercel_project_id_secret] }}
    steps:
      - uses: actions/checkout@v4
      - name: Print version to console
        run: |
          echo "Version number for Preview deployment: ${{ needs.compute-non-secret-vars.outputs.new_version }}"
      - name: Update Version Number in package.json
        working-directory: ./flowmazonfrontend
        run: npm --no-git-tag-version version ${{ needs.compute-non-secret-vars.outputs.new_version }}
      - name: Install Vercel CLI
        working-directory: ./flowmazonfrontend
        run: npm install --global vercel@latest
      - name: Pull Vercel Environment Information
        working-directory: ./flowmazonfrontend
        run: vercel pull --yes --environment=preview --token=${{ secrets.VERCEL_API_TOKEN }}
      - name: Build Project Artifacts
        working-directory: ./flowmazonfrontend
        run: vercel build --prod --token=${{ secrets.VERCEL_API_TOKEN }}
        env:
          NEXT_PUBLIC_BACKEND_URL: https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}
          NEXT_PUBLIC_OTEL_ENVIRONMENT: ${{ needs.compute-non-secret-vars.outputs.otel_env }}
          NEXT_PUBLIC_FARO_URL: ${{ secrets[needs.compute-non-secret-vars.outputs.grafana_instance_url_secret] }}
          NEXT_PUBLIC_FARO_SERVICE_NAME: ${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}

      - name: Deploy Project Artifacts to Vercel
        working-directory: ./flowmazonfrontend
        id: deploy-artifacts
        run: |
          previewUrl=$(vercel deploy --prebuilt --prod --token=${{ secrets.VERCEL_API_TOKEN }})
          echo "previewUrl=$previewUrl" >> "$GITHUB_OUTPUT"

  chromatic:
    uses: "./.github/workflows/chromatic.yml"
    with:
      failOnChanges: false
      workingDir: ./flowmazonfrontend
    secrets:
      CHROMATIC_PROJECT_TOKEN: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}

  comment-preview-links:
    name: Post links to Preview env on PR
    runs-on: ubuntu-24.04
    needs:
      [
        compute-non-secret-vars,
        deploy-backend,
        deploy-frontend-to-vercel,
        playwright-tests,
        chromatic,
      ]
    if: github.event.action != 'closed'
    steps:
      - name: Post sticky comment with preview links
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          recreate: true # so it always appears at the end where PR checks are
          header: preview-environment-links
          # recreate: true
          message: |
            # Preview Environment Links

            - **Web:** https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}

            - **API:** https://${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}

            - **Playwright Test Report:** download from **Artifacts section at the bottom of [this page](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})**

            ## Note

            - Links given above are for the last **successful** deployment to the Preview environment for this pull request. 

                If the latest deployment failed (see Checks section below) then these links could be out of date (i.e. could take you to a previous Preview deployment or may not work at all).

            -  Playwright HTML report would have screenshots, videos and interactive DOM snapshots for failed tests only.

            -  Summary test run reports are available for each test suite (API Unit Tests, API Integration Tests, Playwright Integration Tests) in the Checks section of this pull request. 

                They would be in a random section in Checks. Due to a limitation of the GitHub API, it is not possible to control where in Checks they appear.

  destroy-ephemeral-environment:
    name: Destroy Infrastructure for PR's Preview
    runs-on: ubuntu-24.04
    environment:
      name: Preview
    needs: compute-non-secret-vars
    if: github.event.action == 'closed'
    env:
      # Providers' env requirements
      ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
      ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
      ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
      ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
      NEON_API_KEY: ${{ secrets.NEON_API_KEY }}
      VERCEL_API_TOKEN: ${{ secrets.VERCEL_API_TOKEN }}
      GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

      - name: Destroy API infrastructure (preview-ephemeral-api)
        working-directory: .iac/workspaces/preview-ephemeral-api
        env:
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          terraform init
          terraform destroy -auto-approve \
            -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
            -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
            -var="app_resource_group_name=${{ needs.compute-non-secret-vars.outputs.api_resource_group_name }}" \
            -var="app_resource_group_location=${{ vars.API_RESOURCE_GROUP_LOCATION }}" \
            -var="app_name=${{ needs.compute-non-secret-vars.outputs.api_app_name }}" \
            -var="app_environment_name=${{ needs.compute-non-secret-vars.outputs.api_app_environment_name }}" \
            -var="app_domain_name=${{ needs.compute-non-secret-vars.outputs.api_app_domain_name }}" \
            -var="app_container_name=${{ vars.API_CONTAINER_NAME }}" \
            -var="app_container_port=${{ vars.API_CONTAINER_PORT }}" \
            -var="github_organisation_or_account=${{ github.actor }}" \
            -var="github_token_for_registry_read=${{ secrets.API_APP_GH_TOKEN }}" \
            -var="vault_secretname_registry_password_or_token=${{ needs.compute-non-secret-vars.outputs.vault_secretname_registry_password_or_token }}" \
            -var="image_repository=${{ vars.IMAGE_REPOSITORY }}" \
            -var="image_tag=${{ needs.compute-non-secret-vars.outputs.new_version }}" \
            -var="managed_identity_name=${{ vars.MANAGED_IDENTITY_NAME }}" \
            -var="id_and_vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
            -var="key_vault_name=${{ vars.VAULT_NAME }}" \
            -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
            -var="allowed_cors_origins_for_api=https://${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
            -var="env_OTEL_RESOURCE_ATTRIBUTES=deployment.environment.name=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
            -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
            -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
            -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
            -var="vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS=${{ needs.compute-non-secret-vars.outputs.vault_secretname_env_OTEL_EXPORTER_OTLP_HEADERS }}"
      - name: Destroy preview-ephemeral infrastructure
        working-directory: .iac/workspaces/preview-ephemeral
        env:
          TF_WORKSPACE: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        run: |
          terraform init
          terraform destroy -auto-approve \
            -var="vault_name=${{ vars.VAULT_NAME }}" \
            -var="vault_resource_group_name=${{ vars.ID_AND_VAULT_RESOURCE_GROUP_NAME }}" \
            -var="vault_secretname_for_connectionstring=${{ needs.compute-non-secret-vars.outputs.vault_secret_conn_name }}" \
            -var="secretname_for_psql_owner_connectionstring=${{ needs.compute-non-secret-vars.outputs.psql_owner_conn_name }}" \
            -var="environmentname_for_secrets_and_variables=Preview" \
            -var="repository_for_secrets_and_variables=flowmazondotnet" \
            -var="neon_project_id=${{ secrets.TF_MANAGED_NEON_PROJECT_ID }}" \
            -var="neon_source_branch_id=${{ secrets.TF_MANAGED_NEON_SOURCE_BRANCH_ID }}" \
            -var="neon_new_branch_name=${{ needs.compute-non-secret-vars.outputs.neon_branch_name }}" \
            -var="neon_database_name=${{ vars.NEON_DATABASE_NAME }}" \
            -var="neon_app_role=${{ vars.NEON_APP_ROLE }}" \
            -var="neon_owner_role=${{ vars.NEON_OWNER_ROLE }}" \
            -var="neon_app_role_password=${{ secrets.TF_MANAGED_NEON_APP_ROLE_PASSWORD }}" \
            -var="neon_owner_role_password=${{ secrets.TF_MANAGED_NEON_OWNER_ROLE_PASSWORD }}" \
            -var="vercel_team_id=${{ secrets.VERCEL_TEAM_ID }}" \
            -var="vercel_project_name=${{ needs.compute-non-secret-vars.outputs.vercel_project_name }}" \
            -var="vercel_app_domain_name=${{ needs.compute-non-secret-vars.outputs.vercel_app_domain }}" \
            -var="vercel_region_for_server_side_execution=${{ vars.VERCEL_REGION_FOR_SERVER_SIDE_EXECUTION }}" \
            -var="cloudflare_api_token=${{ secrets.CLOUDFLARE_API_TOKEN }}" \
            -var="cloudflare_zone_id=${{ secrets.CLOUDFLARE_ZONE_ID }}" \
            -var="grafanacloud_stack_slug=${{ vars.GRAFANA_STACK_SLUG }}" \
            -var="grafanacloud_frontend_o11y_api_access_token=${{ secrets.GRAFANA_FRONTEND_O11Y_API_ACCESS_TOKEN }}" \
            -var="grafanacloud_frontend_o11y_instance_name=${{ needs.compute-non-secret-vars.outputs.grafana_instance_name }}" \
            -var="secretname_for_grafanacloud_frontend_o11y_instance_url=${{ needs.compute-non-secret-vars.outputs.grafana_instance_url_secret }}" \
            -var="env_NEXT_PUBLIC_OTEL_ENVIRONMENT=${{ needs.compute-non-secret-vars.outputs.otel_env }}" \
            -var="env_OTEL_EXPORTER_OTLP_ENDPOINT=${{ vars.ENV_OTEL_EXPORTER_OTLP_ENDPOINT }}" \
            -var="env_OTEL_EXPORTER_OTLP_PROTOCOL=${{ vars.ENV_OTEL_EXPORTER_OTLP_PROTOCOL }}" \
            -var="env_OTEL_EXPORTER_OTLP_HEADERS=${{ secrets.ENV_OTEL_EXPORTER_OTLP_HEADERS }}" \
            -var="secretname_for_vercel_project_id=${{ needs.compute-non-secret-vars.outputs.vercel_project_id_secret }}"

      - name: Install jq for JSON parsing
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Delete HCP Terraform workspace (preview-ephemeral-api)
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_api_workspace_name }}
        run: |
          echo "Deleting HCP Terraform workspace: ${WORKSPACE_NAME}"
          # Get workspace by name to obtain its ID
          response=$(curl -s -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            "https://app.terraform.io/api/v2/organizations/${HCP_TF_ORG}/workspaces/${WORKSPACE_NAME}" || echo '{"errors":[]}')

          if echo "$response" | jq -e '.errors' >/dev/null 2>&1; then
            echo "Workspace ${WORKSPACE_NAME} not found or already deleted. Skipping."
            exit 0
          fi

          workspace_id=$(echo "$response" | jq -r '.data.id')
          if [ -z "$workspace_id" ] || [ "$workspace_id" = "null" ]; then
            echo "Could not resolve workspace ID for ${WORKSPACE_NAME}. Skipping."
            exit 0
          fi

          # Delete workspace by ID
          delete_response=$(curl -s -w "%{http_code}" -o /dev/null \
            -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            -X DELETE "https://app.terraform.io/api/v2/workspaces/${workspace_id}")

          if [ "$delete_response" = "200" ] || [ "$delete_response" = "204" ] || [ "$delete_response" = "404" ]; then
            echo "Successfully deleted workspace ${WORKSPACE_NAME} (ID: ${workspace_id})"
          else
            echo "Failed to delete workspace ${WORKSPACE_NAME} (HTTP ${delete_response})"
            exit 1
          fi

      - name: Delete HCP Terraform workspace (preview-ephemeral)
        env:
          HCP_TF_ORG: ${{ secrets.TF_ORG }}
          HCP_TF_TOKEN: ${{ secrets.TF_API_TOKEN }}
          WORKSPACE_NAME: ${{ needs.compute-non-secret-vars.outputs.tf_workspace_name }}
        run: |
          echo "Deleting HCP Terraform workspace: ${WORKSPACE_NAME}"
          # Get workspace by name to obtain its ID
          response=$(curl -s -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            "https://app.terraform.io/api/v2/organizations/${HCP_TF_ORG}/workspaces/${WORKSPACE_NAME}" || echo '{"errors":[]}')

          if echo "$response" | jq -e '.errors' >/dev/null 2>&1; then
            echo "Workspace ${WORKSPACE_NAME} not found or already deleted. Skipping."
            exit 0
          fi

          workspace_id=$(echo "$response" | jq -r '.data.id')
          if [ -z "$workspace_id" ] || [ "$workspace_id" = "null" ]; then
            echo "Could not resolve workspace ID for ${WORKSPACE_NAME}. Skipping."
            exit 0
          fi

          # Delete workspace by ID
          delete_response=$(curl -s -w "%{http_code}" -o /dev/null \
            -H "Authorization: Bearer ${HCP_TF_TOKEN}" \
            -X DELETE "https://app.terraform.io/api/v2/workspaces/${workspace_id}")

          if [ "$delete_response" = "200" ] || [ "$delete_response" = "204" ] || [ "$delete_response" = "404" ]; then
            echo "Successfully deleted workspace ${WORKSPACE_NAME} (ID: ${workspace_id})"
          else
            echo "Failed to delete workspace ${WORKSPACE_NAME} (HTTP ${delete_response})"
            exit 1
          fi
